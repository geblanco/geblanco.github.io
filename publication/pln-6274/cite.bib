@article{PLN6274,
 abstract = {In this work we explore to what extent multilingual models can be trained for one language and applied to a different one for the task of Multiple Choice Question Answering. We employ the RACE dataset to fine-tune both a monolingual and a multilingual models and apply these models to another different collections in different languages. The results show that both monolingual and multilingual models can be zero-shot transferred to a different dataset in the same language maintaining its performance. Besides, the multilingual model still performs good when it is applied to a different target language. Additionally, we find that exams that are more difficult to humans are harder for machines too. Finally, we advance the state-of-the-art for the QA4MRE Entrance Exams dataset in several languages.},
 author = {Guillermo Echegoyen y Álvaro Rodrigo y Anselmo Peñas},
 issn = {1989-7553},
 journal = {Procesamiento del Lenguaje Natural},
 keywords = {},
 number = {0},
 pages = {37--44},
 title = {Cross-lingual Training for Multiple-Choice Question Answering},
 url = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6274},
 volume = {65},
 year = {2020}
}

